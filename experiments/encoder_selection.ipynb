{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9604256b-e45a-4f9c-b03b-3eb8e0868c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import v2\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import clearml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3e6912-abaf-4489-a933-aa9c9d12a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl._create_default_https_context = ssl.create_default_context\n",
    "ssl._create_default_https_context.cafile = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86e8aa8-c5f7-4033-9afe-b728d6022f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=f4b06d6a90d5486aa68f3c4398c40771\n",
      "ClearML results page: https://app.clear.ml/projects/22856c5ce3ac4602a83e933c56a4600a/experiments/f4b06d6a90d5486aa68f3c4398c40771/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "task = clearml.Task.init(project_name=\"Caption generator\", task_name=\"Selecting encoder\", tags=['encoder', 'feature_extraction'])\n",
    "logger = task.get_logger()\n",
    "\n",
    "CONFIG_PATH = './config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98d557-d66d-4e3a-b368-f11247ec4e6a",
   "metadata": {},
   "source": [
    "## Encoders factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ceff9b1-d4d5-4fa8-8c9c-9fc83e2da125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim=None\n",
    "        self.features_dim=None\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def get_features_dim(self):\n",
    "        return self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5cdeb81-e93f-4995-a9c0-7b8721a9a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(BaseEncoder):\n",
    "    def __init__(self, embedding_dim=256, pretrained=True):\n",
    "        super().__init__(embedding_dim)\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        self.feature_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.flatten(features, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7b4455-68a9-43af-af12-57177077339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet101(BaseEncoder):\n",
    "    def __init__(self, embedding_dim=256, pretrained=True):\n",
    "        super().__init__(embedding_dim)\n",
    "        model = models.resnet101(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        self.feature_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.flatten(features, x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7516326a-a7ac-49a8-8bfe-469ed8a49d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(BaseEncoder):\n",
    "    def __init__(self, embedding_dim=256, model_name='efficientnet_b0', pretrained=True):\n",
    "        super().__init__(embedding_dim)\n",
    "        weights = 'DEFAULT' if pretrained else None\n",
    "        if model_name == 'efficientnet_b0':\n",
    "            model = models.efficientnet_b0(weights=weights)\n",
    "        elif model_name == 'efficientnet_b4':\n",
    "            model = models.efficientnet_b4(weights=weights)\n",
    "        else:\n",
    "            raise ValueError('Unknown model (efficient net)')\n",
    "\n",
    "        self.feature_dim = 1280 if 'b0' in model_name else 1792\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.flatten(features, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a65dc5-1ad8-4ae8-8481-5281a63ffdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderFactory:\n",
    "    @staticmethod\n",
    "    def create_encoder(encoder_name, embedding_dim=256, pretrained=True):\n",
    "        all_encoders = {\n",
    "            \"resnet50\": ResNet50,\n",
    "            \"resnet101\": ResNet101,\n",
    "            \"efficientnet_b0\": lambda **kwargs: EfficientNet(model_name=\"efficientnet_b0\", **kwargs),\n",
    "            \"efficientnet_b4\": lambda **kwargs: EfficientNet(model_name=\"efficientnet_b4\", **kwargs)\n",
    "        }\n",
    "        selected_encoder = all_encoders[encoder_name](\n",
    "            embedding_dim=embedding_dim,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        return selected_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4e0e10-7bda-42fa-a793-50e352a74b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import urllib.request as ur\n",
    "ur._opener = None\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f93292-2834-4be0-aa03-952a34542b9e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8d1790-8a47-42e4-aec9-29d7475fbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CONFIG_PATH, 'r') as config:\n",
    "    cfg = yaml.safe_load(config)\n",
    "\n",
    "DATA_PATH  = cfg['dataset']['path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5429f9d6-e6a1-4734-a0ef-c2278add1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583087629_a09334e1fb.jpg | 2641770481_c98465ff35.jpg | 530950375_eea665583f.jpg | 2872197070_4e97c3ccfa.jpg | 2369452202_8b0e8e25ca.jpg | 2789937754_5d1fa62e95.jpg | 543326592_70bd4d8602.jpg | 3173461705_b5cdeef1eb.jpg | 528500099_7be78a0ca5.jpg | 353180303_6a24179c50.jpg | "
     ]
    }
   ],
   "source": [
    "for picture in os.listdir(os.path.join(DATA_PATH, \"images\"))[:10]:\n",
    "    print(picture, end = ' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "307fda85-57c0-4189-8552-3ea26eb9a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataset_size, data_path=DATA_PATH, transform=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = os.path.join(data_path, \"images\")\n",
    "        self.labels = os.path.join(data_path, \"captions.txt\")\n",
    "        self.image_caption = []\n",
    "        self._preprocess_labels()\n",
    "\n",
    "\n",
    "        self.image_caption = self.image_caption[:dataset_size]\n",
    "        self.images_idx = [pair[0] for pair in self.image_caption]\n",
    "        self.labels_idx = [pair[1] for pair in self.image_caption]\n",
    "    \n",
    "\n",
    "    def _preprocess_labels(self):\n",
    "        with open(self.labels, 'r', encoding='utf-8') as f: \n",
    "            next(f) #first row contains image,caption info\n",
    "            for line in f:\n",
    "                filename, label = line.strip().split(\",\", 1)\n",
    "                tokens = word_tokenize(label)\n",
    "                self.image_caption.append((filename, tokens))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images_idx[idx]\n",
    "        label = self.labels_idx[idx]\n",
    "\n",
    "        image = Image.open(os.path.join(self.data_path, \"images\", image_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66544798-7cf4-41cb-9a3d-77995e439589",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_file = os.path.join(DATA_PATH, 'images', os.listdir(os.path.join(DATA_PATH, 'images'))[0])\n",
    "\n",
    "image_size = Image.open(first_file).convert('RGB').size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d37533-4dec-4ead-b7fb-dc25f4405b7a",
   "metadata": {},
   "source": [
    "## Class for all needed tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07c837f5-3295-40c9-b0b5-8d0e7c5771ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy import stats as scipy_stats\n",
    "\n",
    "class ComparisonTests:\n",
    "    @staticmethod\n",
    "    def num_parameters(model, return_trainable=True):\n",
    "        total_params, trainable_params = [], []\n",
    "        for p in model.parameters():\n",
    "            total_params.append(p.numel())\n",
    "            if return_trainable and p.requires_grad:\n",
    "                trainable_params.append(p.numel())\n",
    "\n",
    "        total_sum, trainable_sum = sum(total_params), sum(trainable_params)\n",
    "        total_trainable_ratio = trainable_sum / total_sum\n",
    "\n",
    "        return {\n",
    "            'total_params': total_sum,\n",
    "            'trainable_params': trainable_sum,\n",
    "            'total_trainable_ratio': total_trainable_ratio\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def model_inference_time(model, test_loader, device, n_batches):\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, _) in enumerate(test_loader):\n",
    "                if idx >= n_batches:\n",
    "                    break\n",
    "\n",
    "                start = time.time()\n",
    "                _ = model(images)\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "\n",
    "                times.append(end - start)\n",
    "\n",
    "        sum_time = sum(time)\n",
    "        mean_time = np.mean(time)\n",
    "        batch_shape = images.shape[0]\n",
    "\n",
    "        return {\n",
    "            'sum_time_ms': sum_time * 1000,\n",
    "            'mean_time_ms': mean_time * 1000,\n",
    "            'throughput_images_per_sec': batch_shape / mean_time\n",
    "        }\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def model_memory_usage(model, device='cuda'):\n",
    "        model = model.to(device)\n",
    "        param_memory = sum(p.numel() * 4 for p in model.parameters()) / (1024 ** 2)\n",
    "        buffer_memory = sum(b.numel() * 4 for b in model.buffers()) / (1024 ** 2)\n",
    "        dummy_input = torch.randn(32, 3, 224, 224).to(device)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "        \n",
    "        activation_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'param_memory_mb': param_memory,\n",
    "            'buffer_memory_mb': buffer_memory,\n",
    "            'activation_memory_mb': activation_memory,\n",
    "            'total_memory_mb': param_memory + buffer_memory + activation_memory\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_main_statistics(model, test_loader, device='cuda', n_batches=10):\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "        features = []\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, _) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                \n",
    "                f = model(images)\n",
    "                f = f.view(f.size(0), -1)\n",
    "                features.append(f.cpu())\n",
    "                \n",
    "                if idx >= n_batches - 1:\n",
    "                    break\n",
    "    \n",
    "        features = torch.cat(features, dim=0)\n",
    "        stats = {}\n",
    "    \n",
    "        var_per_dim = torch.var(features, dim=0)\n",
    "        stats['var_mean'] = var_per_dim.mean().item()\n",
    "        stats['dead_features_ratio'] = (var_per_dim < 1e-5).float().mean().item()\n",
    "    \n",
    "        corr_matrix = torch.corrcoef(features.T)\n",
    "        mask = ~torch.eye(corr_matrix.size(0), dtype=bool)\n",
    "        stats['mean_abs_corr'] = corr_matrix[mask].abs().mean().item()\n",
    "    \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "        ax = axes[0, 0]\n",
    "        ax.hist(features.flatten().numpy(), bins=100, density=True, alpha=.7)\n",
    "        ax.set_title('Distribution of All Feature Values')\n",
    "        ax.set_xlabel('Feature Value')\n",
    "        ax.set_ylabel('Density')\n",
    "        ax.axvline(features.mean().item(), color='r', linestyle='--', label=f'Mean: {features.mean().item():.2f}')\n",
    "        ax.axvline(features.median().item(), color='g', linestyle='--', label=f'Median: {features.median().item():.2f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        ax = axes[0, 1]\n",
    "        dim_means = features.mean(dim=0).numpy()\n",
    "        ax.hist(dim_means, bins=50, alpha=0.7, edgecolor='black')\n",
    "        ax.set_title('Distribution of Per-Dimension Means')\n",
    "        ax.set_xlabel('Mean Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        \n",
    "        ax = axes[1, 0]\n",
    "        dim_vars = torch.var(features, dim=0).numpy()\n",
    "        ax.hist(dim_vars, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "        ax.set_title('Distribution of Per-Dimension Variance')\n",
    "        ax.set_xlabel('Variance')\n",
    "        ax.set_ylabel('Count')\n",
    "        \n",
    "        ax = axes[1, 1]\n",
    "        scipy_stats.probplot(features.flatten().numpy(), dist=\"norm\", plot=ax)\n",
    "        ax.set_title('QQ-plot')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return stats, features, fig\n",
    "\n",
    "    @classmethod\n",
    "    def comparion_tests(cls, model, test_loader, device='cuda', return_trainable=True, n_batches=10):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25b0ff-8a0f-4b2c-9db5-9f314a9184ff",
   "metadata": {},
   "source": [
    "Computing optimal mean / std for each channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfbb500-804a-44a2-b4d2-89f25e87c9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_std(dataset, batch_size, num_workers, collate_fn):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=collate_fn)\n",
    "\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    total_samples = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, 3, -1)\n",
    "\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "\n",
    "        total_samples += batch_samples\n",
    "\n",
    "    mean /= total_samples\n",
    "    std /= total_samples\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f1d733c-b9e7-4d90-8301-f3c50fd37b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch])\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81a53c57-8e0a-436f-98bb-0b8ef6a9ef6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal mean: tensor([0.4463, 0.4506, 0.4166]) | Optimal std: tensor([0.2235, 0.2138, 0.2222])\n"
     ]
    }
   ],
   "source": [
    "transform_row = v2.Compose([\n",
    "    v2.Resize(size=(224, 224)),\n",
    "    v2.ToTensor()\n",
    "])\n",
    "\n",
    "total_size = cfg['dataset']['train_size'] + cfg['dataset']['test_size']\n",
    "dataset = FlickrDataset(dataset_size=total_size, data_path=cfg['dataset']['path'], transform=transform_row)\n",
    "optimal_mean, optimal_std = compute_mean_std(\n",
    "    dataset, batch_size=cfg['evaluation']['batch_size'], num_workers=cfg['evaluation']['num_workers'], collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Optimal mean: {} | Optimal std: {}\".format(optimal_mean, optimal_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24d3e895-01a3-4bac-b136-3673dde2ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = cfg['dataset']['train_size'] + cfg['dataset']['test_size']\n",
    "transform = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=optimal_mean, std=optimal_std)\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(dataset_size=total_size, data_path=cfg['dataset']['path'], transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9d5af-9715-44a1-9b1e-25042460024a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
