{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9604256b-e45a-4f9c-b03b-3eb8e0868c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import clearml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e3e6912-abaf-4489-a933-aa9c9d12a739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "import certifi\n",
    "\n",
    "ssl._create_default_https_context = ssl.create_default_context\n",
    "ssl._create_default_https_context.cafile = certifi.where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e86e8aa8-c5f7-4033-9afe-b728d6022f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=53aa50a075cb417fbe29d42d48ab7d18\n",
      "ClearML results page: https://app.clear.ml/projects/22856c5ce3ac4602a83e933c56a4600a/experiments/53aa50a075cb417fbe29d42d48ab7d18/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "task = clearml.Task.init(project_name=\"Caption generator\", task_name=\"Selecting encoder\", tags=['encoder', 'feature_extraction'])\n",
    "logger = task.get_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98d557-d66d-4e3a-b368-f11247ec4e6a",
   "metadata": {},
   "source": [
    "## Encoders factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ceff9b1-d4d5-4fa8-8c9c-9fc83e2da125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=None):\n",
    "        super().__init__()\n",
    "        self.embedding_dim=None\n",
    "        self.features_dim=None\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def get_features_dim(self):\n",
    "        return self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5cdeb81-e93f-4995-a9c0-7b8721a9a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(BaseEncoder):\n",
    "    def __init__(self, embedding_dim=256, pretrained=True):\n",
    "        super().__init__(embedding_dim)\n",
    "        model = models.resnet50(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        self.feature_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.flatten(features, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7b4455-68a9-43af-af12-57177077339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet101(BaseEncoder):\n",
    "    def __init__(self, embedding_dim=256, pretrained=True):\n",
    "        super().__init__(embedding_dim)\n",
    "        model = models.resnet101(pretrained=pretrained)\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "        self.feature_dim = 2048\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.flatten(features, x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7516326a-a7ac-49a8-8bfe-469ed8a49d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNet(BaseEncoder):\n",
    "    def __init__(self, embedding_dim=256, model_name='efficientnet_b0', pretrained=True):\n",
    "        super().__init__(embedding_dim)\n",
    "        weights = 'DEFAULT' if pretrained else None\n",
    "        if model_name == 'efficientnet_b0':\n",
    "            model = models.efficientnet_b0(weights=weights)\n",
    "        elif model_name == 'efficientnet_b4':\n",
    "            model = models.efficientnet_b4(weights=weights)\n",
    "        else:\n",
    "            raise ValueError('Unknown model (efficient net)')\n",
    "\n",
    "        self.feature_dim = 1280 if 'b0' in model_name else 1792\n",
    "        self.features = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = torch.flatten(features, 1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a65dc5-1ad8-4ae8-8481-5281a63ffdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderFactory:\n",
    "    @staticmethod\n",
    "    def create_encoder(encoder_name, embedding_dim=256, pretrained=True):\n",
    "        all_encoders = {\n",
    "            \"resnet50\": ResNet50,\n",
    "            \"resnet101\": ResNet101,\n",
    "            \"efficientnet_b0\": lambda **kwargs: EfficientNet(model_name=\"efficientnet_b0\", **kwargs),\n",
    "            \"efficientnet_b4\": lambda **kwargs: EfficientNet(model_name=\"efficientnet_b4\", **kwargs)\n",
    "        }\n",
    "        selected_encoder = all_encoders[encoder_name](\n",
    "            embedding_dim=embedding_dim,\n",
    "            pretrained=pretrained\n",
    "        )\n",
    "\n",
    "        return selected_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4e0e10-7bda-42fa-a793-50e352a74b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ssl\n",
    "import urllib.request\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import urllib.request as ur\n",
    "ur._opener = None\n",
    "os.environ['CURL_CA_BUNDLE'] = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f93292-2834-4be0-aa03-952a34542b9e",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8d1790-8a47-42e4-aec9-29d7475fbdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captions.txt', 'images']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"../data\"\n",
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5429f9d6-e6a1-4734-a0ef-c2278add1d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "583087629_a09334e1fb.jpg | 2641770481_c98465ff35.jpg | 530950375_eea665583f.jpg | 2872197070_4e97c3ccfa.jpg | 2369452202_8b0e8e25ca.jpg | 2789937754_5d1fa62e95.jpg | 543326592_70bd4d8602.jpg | 3173461705_b5cdeef1eb.jpg | 528500099_7be78a0ca5.jpg | 353180303_6a24179c50.jpg | "
     ]
    }
   ],
   "source": [
    "for picture in os.listdir(os.path.join(DATA_PATH, \"images\"))[:10]:\n",
    "    print(picture, end = ' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de770dd-cb43-435a-aa47-94b1593b6125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "CONFIG_FILE = \"./config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "307fda85-57c0-4189-8552-3ea26eb9a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, dataset_size, data_path=DATA_PATH, transform=None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "\n",
    "        self.images = os.path.join(data_path, \"images\")\n",
    "        self.labels = os.path.join(data_path, \"captions.txt\")\n",
    "        self.image_caption = {}\n",
    "        self._preprocess_labels()\n",
    "\n",
    "        self.images_idx = list(self.image_caption.keys())[:dataset_size]\n",
    "        self.labels_idx = list(self.image_caption.values())[:dataset_size]\n",
    "\n",
    "    def _preprocess_labels(self):\n",
    "        with open(self.labels, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                filename, label = line.strip().split(\",\", 1)\n",
    "                tokens = word_tokenize(label)\n",
    "                self.image_caption[filename] = tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.images_idx[idx]\n",
    "        label = self.labels_idx[idx]\n",
    "\n",
    "        image = Image.open(os.path.join(self.data_path, \"images\", image_name)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d94bbec4-773a-4c81-8225-e58ba94f4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as config:\n",
    "    cfg = yaml.safe_load(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66544798-7cf4-41cb-9a3d-77995e439589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 375)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_file = os.path.join(DATA_PATH, 'images', os.listdir(os.path.join(DATA_PATH, 'images'))[0])\n",
    "\n",
    "first_image = Image.open(first_file).convert('RGB')\n",
    "first_image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c7028e92-7553-417b-ba7a-2ccd9dfb59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2\n",
    "import time\n",
    "\n",
    "total_size = cfg['dataset']['train_size'] + cfg['dataset']['test_size']\n",
    "transform = v2.Compose([\n",
    "    v2.RandomResizedCrop(size=(224, 224), antialias=True),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(dataset_size=total_size, data_path=cfg['dataset']['path'], transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d37533-4dec-4ead-b7fb-dc25f4405b7a",
   "metadata": {},
   "source": [
    "## Class for all needed tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07c837f5-3295-40c9-b0b5-8d0e7c5771ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparisonTests:\n",
    "    @staticmethod\n",
    "    def num_parameters(model, return_trainable=True):\n",
    "        total_params, trainable_params = [], []\n",
    "        for p in model.parameters():\n",
    "            total_params.append(p.numel())\n",
    "            if return_trainable and p.requires_grad:\n",
    "                trainable_params.append(p.numel())\n",
    "\n",
    "        total_sum, trainable_sum = sum(total_params), sum(trainable_params)\n",
    "        total_trainable_ratio = trainable_sum / total_sum\n",
    "\n",
    "        return {\n",
    "            'total_params': total_sum,\n",
    "            'trainable_params': trainable_sum,\n",
    "            'total_trainable_ratio': total_trainable_ratio\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def model_inference_time(model, test_loader, device, n_batches):\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        times = []\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, _) in enumerate(test_loader):\n",
    "                if idx >= n_batches:\n",
    "                    break\n",
    "\n",
    "                start = time.time()\n",
    "                _ = model(images)\n",
    "                torch.cuda.synchronize()\n",
    "                end = time.time()\n",
    "\n",
    "                times.append(end - start)\n",
    "\n",
    "        sum_time = sum(time)\n",
    "        mean_time = np.mean(time)\n",
    "        batch_shape = images.shape[0]\n",
    "\n",
    "        return {\n",
    "            'sum_time_ms': sum_time * 1000,\n",
    "            'mean_time_ms': mean_time * 1000,\n",
    "            'throughput_images_per_sec': batch_shape / mean_time\n",
    "        }\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def model_memory_usage(model, device='cuda'):\n",
    "        model = model.to(device)\n",
    "        param_memory = sum(p.numel() * 4 for p in model.parameters()) / (1024 ** 2)\n",
    "        buffer_memory = sum(b.numel() * 4 for b in model.buffers()) / (1024 ** 2)\n",
    "        dummy_input = torch.randn(32, 3, 224, 224).to(device)\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model(dummy_input)\n",
    "        \n",
    "        activation_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "        \n",
    "        return {\n",
    "            'param_memory_mb': param_memory,\n",
    "            'buffer_memory_mb': buffer_memory,\n",
    "            'activation_memory_mb': activation_memory,\n",
    "            'total_memory_mb': param_memory + buffer_memory + activation_memory\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_main_statistics(model, test_loader, device='cuda', n_batches=10):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def comparion_tests(cls, model, test_loader, device='cuda', return_trainable=True, n_batches=10):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
